# AI Assistant API

Nowoczesny asystent AI wykorzystujÄ…cy lokalne modele LLM, zbudowany w Python z FastAPI i Svelte.

## ğŸš€ Funkcje

- ğŸ’¬ Interaktywny chat z AI
- ğŸ” Wyszukiwanie semantyczne z RAG
- ğŸ“š ObsÅ‚uga dokumentÃ³w i kontekstu
- ğŸ” Uwierzytelnianie i autoryzacja
- ğŸŒ Interfejs WebSocket do komunikacji w czasie rzeczywistym
- ğŸ“Š Monitorowanie i logowanie
- ğŸ§ª Testy jednostkowe i integracyjne

## ğŸ› ï¸ Technologie

### Backend
- Python 3.9+
- FastAPI
- Pydantic
- LangChain
- FAISS
- Sentence Transformers
- Ollama

### Frontend
- Svelte
- TypeScript
- TailwindCSS
- WebSocket

## ğŸ“‹ Wymagania

- Python 3.9+
- Node.js 16+
- Ollama (lokalnie lub zdalnie)
- 8GB+ RAM (zalecane)
- GPU (opcjonalnie, dla lepszej wydajnoÅ›ci)

## ğŸš€ Instalacja

1. Sklonuj repozytorium:
```bash
git clone https://github.com/yourusername/ai-assistant.git
cd ai-assistant
```

2. UtwÃ³rz i aktywuj Å›rodowisko wirtualne:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# lub
.\venv\Scripts\activate  # Windows
```

3. Zainstaluj zaleÅ¼noÅ›ci:
```bash
pip install -r requirements.txt
```

4. Skonfiguruj zmienne Å›rodowiskowe:
```bash
cp .env.example .env
# Edytuj .env zgodnie z potrzebami
```

5. Uruchom skrypt instalacyjny:
```bash
./setup.sh
```

## ğŸƒâ€â™‚ï¸ Uruchomienie

1. Uruchom backend:
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

2. Uruchom frontend (w osobnym terminalu):
```bash
cd frontend
npm install
npm run dev
```

3. OtwÃ³rz przeglÄ…darkÄ™:
```
http://localhost:8000
```

## ğŸ“š Dokumentacja API

DostÄ™pne endpointy:

- `GET /` - Strona gÅ‚Ã³wna
- `GET /api/health` - Sprawdzenie stanu API
- `GET /api/models` - Lista dostÄ™pnych modeli AI
- `POST /api/chat` - Endpoint do wysyÅ‚ania wiadomoÅ›ci
- `WS /ws/{client_id}` - WebSocket do komunikacji w czasie rzeczywistym

PeÅ‚na dokumentacja API dostÄ™pna pod adresem:
```
http://localhost:8000/docs
```

## ğŸ§ª Testy

```bash
# Uruchom wszystkie testy
pytest

# Uruchom testy z pokryciem
pytest --cov=.

# Uruchom konkretny test
pytest tests/test_specific.py
```

## ğŸ“ Struktura Projektu

```
project/
â”œâ”€â”€ core/                 # RdzeÅ„ aplikacji
â”‚   â”œâ”€â”€ ai_engine.py     # GÅ‚Ã³wna logika AI
â”‚   â”œâ”€â”€ llm_manager.py   # ZarzÄ…dzanie modelami
â”‚   â”œâ”€â”€ rag_manager.py   # Retrieval-Augmented Generation
â”‚   â””â”€â”€ conversation.py  # ObsÅ‚uga rozmÃ³w
â”œâ”€â”€ interfaces/          # Interfejsy uÅ¼ytkownika
â”‚   â”œâ”€â”€ web_ui.py       # FastAPI web interface
â”‚   â”œâ”€â”€ cli.py          # Command line interface
â”‚   â””â”€â”€ api.py          # REST API endpoints
â”œâ”€â”€ config/             # Konfiguracja
â”œâ”€â”€ utils/              # NarzÄ™dzia pomocnicze
â”œâ”€â”€ static/             # Pliki statyczne (CSS, JS)
â”œâ”€â”€ tests/              # Testy
â””â”€â”€ frontend/           # Frontend Svelte
```

## ğŸ”§ Konfiguracja

GÅ‚Ã³wne ustawienia w pliku `.env`:

```env
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=300

# Models
PRIMARY_MODEL=gemma3:12b
DOCUMENT_MODEL=SpeakLeash/bielik-11b-v2.3-instruct:Q6_K

# RAG Settings
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5
TRUST_REMOTE_CODE=true

# Limits
MAX_FILE_SIZE=52428800  # 50MB
MAX_CONVERSATION_LENGTH=100
REQUEST_TIMEOUT=30

# Paths
UPLOAD_DIR=./uploads
FAISS_INDEX_PATH=./data/faiss_index
```

## ğŸ¤ WspÃ³Å‚praca

1. Fork projektu
2. UtwÃ³rz branch dla nowej funkcji (`git checkout -b feature/amazing-feature`)
3. Commit zmian (`git commit -m 'Add amazing feature'`)
4. Push do brancha (`git push origin feature/amazing-feature`)
5. OtwÃ³rz Pull Request

## ğŸ“ Licencja

Ten projekt jest udostÄ™pniany na licencji MIT. SzczegÃ³Å‚y w pliku [LICENSE](LICENSE).

## ğŸ™ PodziÄ™kowania

- [Ollama](https://github.com/ollama/ollama) za Å›wietne narzÄ™dzie do lokalnych modeli LLM
- [LangChain](https://github.com/langchain-ai/langchain) za framework do aplikacji AI
- [FastAPI](https://github.com/tiangolo/fastapi) za nowoczesny framework webowy
- [Svelte](https://github.com/sveltejs/svelte) za Å›wietny framework frontendowy 