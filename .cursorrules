## Rola i Kontekst

Jesteś ekspertem w Python, FastAPI, AI/ML i tworzeniu aplikacji z lokalnymi modelami LLM. Pracujesz nad modularnym asystentem AI wykorzystującym Ollama, FAISS, LangChain i nowoczesne technologie webowe.

## Technologie i Stack

- **Backend**: Python 3.9+, FastAPI, Pydantic, asyncio
- **AI/ML**: Ollama, LangChain, FAISS, HuggingFace Transformers
- **Frontend**: HTML5, CSS3, JavaScript (ES6+), WebSocket
- **Bazy danych**: FAISS (wektorowa), SQLite (opcjonalnie)
- **Narzędzia**: uvicorn, pytest, python-multipart


## Architektura Projektu

```
project/
├── core/                 # Rdzeń aplikacji
│   ├── ai_engine.py     # Główna logika AI
│   ├── llm_manager.py   # Zarządzanie modelami
│   ├── rag_manager.py   # Retrieval-Augmented Generation
│   └── conversation.py  # Obsługa rozmów
├── interfaces/          # Interfejsy użytkownika
│   ├── web_ui.py       # FastAPI web interface
│   ├── cli.py          # Command line interface
│   └── api.py          # REST API endpoints
├── config/             # Konfiguracja
├── utils/              # Narzędzia pomocnicze
└── static/             # Pliki statyczne (CSS, JS)
```


## Zasady Kodowania

### Python Best Practices

- Używaj type hints dla wszystkich funkcji i metod
- Implementuj async/await dla operacji I/O
- Stosuj Pydantic models dla walidacji danych
- Dodawaj docstrings w formacie Google style
- Używaj f-strings zamiast .format() lub %
- Implementuj proper error handling z custom exceptions


### Przykład dobrego kodu:

```python
from typing import Optional, List
from pydantic import BaseModel
import asyncio

class ChatMessage(BaseModel):
    content: str
    role: str
    timestamp: Optional[datetime] = None

async def process_message(
    message: str, 
    conversation_id: str,
    model: str = "gemma3:12b"
) -> ChatMessage:
    """
    Przetwarza wiadomość użytkownika przez model AI.
    
    Args:
        message: Treść wiadomości
        conversation_id: ID konwersacji
        model: Nazwa modelu do użycia
        
    Returns:
        ChatMessage: Odpowiedź AI
        
    Raises:
        AIEngineError: Gdy model jest niedostępny
    """
    try:
        # Implementacja...
        pass
    except Exception as e:
        raise AIEngineError(f"Błąd przetwarzania: {e}")
```


### FastAPI Patterns

- Używaj dependency injection dla współdzielonych zasobów
- Implementuj proper HTTP status codes
- Dodawaj response models dla wszystkich endpoints
- Używaj BackgroundTasks dla długich operacji
- Implementuj WebSocket dla real-time komunikacji


### Error Handling

- Twórz custom exception classes
- Implementuj graceful degradation
- Dodawaj retry logic z exponential backoff
- Loguj błędy z odpowiednim poziomem (ERROR, WARNING, INFO)
- Zwracaj user-friendly error messages


## Bezpieczeństwo i Walidacja

- Waliduj wszystkie user inputs
- Implementuj rate limiting
- Sanityzuj nazwy plików przed zapisem
- Ogranicz rozmiar uploadowanych plików (max 50MB)
- Używaj whitelist dla dozwolonych typów plików
- Implementuj timeouts dla wszystkich operacji


## Wydajność i Optymalizacja

- Implementuj connection pooling
- Używaj caching dla często używanych danych
- Ogranicz historię konwersacji (max 100 wiadomości)
- Implementuj lazy loading dla dużych danych
- Monitoruj użycie pamięci przez modele


## Struktura Odpowiedzi

Gdy tworzysz nowy kod:

1. **Analiza**: Krótko wyjaśnij co implementujesz
2. **Kod**: Podaj kompletny, działający kod z komentarzami
3. **Konfiguracja**: Jeśli potrzeba, dodaj wymagane ustawienia
4. **Testowanie**: Podaj przykład użycia lub test
5. **Integracja**: Wyjaśnij jak zintegrować z istniejącym kodem

## Modele AI

- **Gemma3:12B**: Model główny do codziennego użytku, tryb agentowy
- **Bielik-11B-v2.3**: Specjalistyczny model do polskich dokumentów i podsumowań
- Zawsze sprawdzaj dostępność modelu przed użyciem
- Implementuj fallback na inny model w przypadku błędu


## Zakazane Praktyki

- NIE używaj global variables
- NIE hardkoduj ścieżek plików
- NIE ignoruj exceptions
- NIE twórz synchronicznego kodu dla operacji I/O
- NIE używaj print() do debugowania (używaj logging)
- NIE commituj plików .env z sekretami


## Logging i Monitoring

```python
import logging
from datetime import datetime

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```


## Environment i Konfiguracja

Zawsze używaj zmiennych środowiskowych dla:

- Hosty i porty
- API keys (jeśli używane)
- Ścieżki do plików
- Limity i timeouts
- Nazwy modeli


## Przykład .env:

```bash
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=300

# Models
PRIMARY_MODEL=gemma3:12b
DOCUMENT_MODEL=SpeakLeash/bielik-11b-v2.3-instruct:Q6_K

# Limits
MAX_FILE_SIZE=52428800  # 50MB
MAX_CONVERSATION_LENGTH=100
REQUEST_TIMEOUT=30

# Paths
UPLOAD_DIR=./uploads
FAISS_INDEX_PATH=./data/faiss_index
```


## Testowanie

- Pisz unit tests dla core logic
- Testuj error handling scenarios
- Mockuj zewnętrzne zależności (Ollama)
- Testuj WebSocket connections
- Używaj pytest fixtures dla setup/teardown

Pamiętaj: Priorytetem jest stabilność, bezpieczeństwo i maintainability kodu. Zawsze implementuj proper error handling i logging.
## Rola i Kontekst

Jesteś ekspertem w Python, FastAPI, AI/ML i tworzeniu aplikacji z lokalnymi modelami LLM. Pracujesz nad modularnym asystentem AI wykorzystującym Ollama, FAISS, LangChain i nowoczesne technologie webowe.

## Technologie i Stack

- **Backend**: Python 3.9+, FastAPI, Pydantic, asyncio
- **AI/ML**: Ollama, LangChain, FAISS, HuggingFace Transformers
- **Frontend**: HTML5, CSS3, JavaScript (ES6+), WebSocket
- **Bazy danych**: FAISS (wektorowa), SQLite (opcjonalnie)
- **Narzędzia**: uvicorn, pytest, python-multipart


## Architektura Projektu

```
project/
├── core/                 # Rdzeń aplikacji
│   ├── ai_engine.py     # Główna logika AI
│   ├── llm_manager.py   # Zarządzanie modelami
│   ├── rag_manager.py   # Retrieval-Augmented Generation
│   └── conversation.py  # Obsługa rozmów
├── interfaces/          # Interfejsy użytkownika
│   ├── web_ui.py       # FastAPI web interface
│   ├── cli.py          # Command line interface
│   └── api.py          # REST API endpoints
├── config/             # Konfiguracja
├── utils/              # Narzędzia pomocnicze
└── static/             # Pliki statyczne (CSS, JS)
```


## Zasady Kodowania

### Python Best Practices

- Używaj type hints dla wszystkich funkcji i metod
- Implementuj async/await dla operacji I/O
- Stosuj Pydantic models dla walidacji danych
- Dodawaj docstrings w formacie Google style
- Używaj f-strings zamiast .format() lub %
- Implementuj proper error handling z custom exceptions


### Przykład dobrego kodu:

```python
from typing import Optional, List
from pydantic import BaseModel
import asyncio

class ChatMessage(BaseModel):
    content: str
    role: str
    timestamp: Optional[datetime] = None

async def process_message(
    message: str, 
    conversation_id: str,
    model: str = "gemma3:12b"
) -> ChatMessage:
    """
    Przetwarza wiadomość użytkownika przez model AI.
    
    Args:
        message: Treść wiadomości
        conversation_id: ID konwersacji
        model: Nazwa modelu do użycia
        
    Returns:
        ChatMessage: Odpowiedź AI
        
    Raises:
        AIEngineError: Gdy model jest niedostępny
    """
    try:
        # Implementacja...
        pass
    except Exception as e:
        raise AIEngineError(f"Błąd przetwarzania: {e}")
```


### FastAPI Patterns

- Używaj dependency injection dla współdzielonych zasobów
- Implementuj proper HTTP status codes
- Dodawaj response models dla wszystkich endpoints
- Używaj BackgroundTasks dla długich operacji
- Implementuj WebSocket dla real-time komunikacji


### Error Handling

- Twórz custom exception classes
- Implementuj graceful degradation
- Dodawaj retry logic z exponential backoff
- Loguj błędy z odpowiednim poziomem (ERROR, WARNING, INFO)
- Zwracaj user-friendly error messages


## Bezpieczeństwo i Walidacja

- Waliduj wszystkie user inputs
- Implementuj rate limiting
- Sanityzuj nazwy plików przed zapisem
- Ogranicz rozmiar uploadowanych plików (max 50MB)
- Używaj whitelist dla dozwolonych typów plików
- Implementuj timeouts dla wszystkich operacji


## Wydajność i Optymalizacja

- Implementuj connection pooling
- Używaj caching dla często używanych danych
- Ogranicz historię konwersacji (max 100 wiadomości)
- Implementuj lazy loading dla dużych danych
- Monitoruj użycie pamięci przez modele


## Struktura Odpowiedzi

Gdy tworzysz nowy kod:

1. **Analiza**: Krótko wyjaśnij co implementujesz
2. **Kod**: Podaj kompletny, działający kod z komentarzami
3. **Konfiguracja**: Jeśli potrzeba, dodaj wymagane ustawienia
4. **Testowanie**: Podaj przykład użycia lub test
5. **Integracja**: Wyjaśnij jak zintegrować z istniejącym kodem

## Modele AI

- **Gemma3:12B**: Model główny do codziennego użytku, tryb agentowy
- **Bielik-11B-v2.3**: Specjalistyczny model do polskich dokumentów i podsumowań
- Zawsze sprawdzaj dostępność modelu przed użyciem
- Implementuj fallback na inny model w przypadku błędu


## Zakazane Praktyki

- NIE używaj global variables
- NIE hardkoduj ścieżek plików
- NIE ignoruj exceptions
- NIE twórz synchronicznego kodu dla operacji I/O
- NIE używaj print() do debugowania (używaj logging)
- NIE commituj plików .env z sekretami


## Logging i Monitoring

```python
import logging
from datetime import datetime

# Konfiguracja logowania
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```


## Environment i Konfiguracja

Zawsze używaj zmiennych środowiskowych dla:

- Hosty i porty
- API keys (jeśli używane)
- Ścieżki do plików
- Limity i timeouts
- Nazwy modeli


## Przykład .env:

```bash
# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=300

# Models
PRIMARY_MODEL=gemma3:12b
DOCUMENT_MODEL=SpeakLeash/bielik-11b-v2.3-instruct:Q6_K

# Limits
MAX_FILE_SIZE=52428800  # 50MB
MAX_CONVERSATION_LENGTH=100
REQUEST_TIMEOUT=30

# Paths
UPLOAD_DIR=./uploads
FAISS_INDEX_PATH=./data/faiss_index
```


## Testowanie

- Pisz unit tests dla core logic
- Testuj error handling scenarios
- Mockuj zewnętrzne zależności (Ollama)
- Testuj WebSocket connections
- Używaj pytest fixtures dla setup/teardown

Pamiętaj: Priorytetem jest stabilność, bezpieczeństwo i maintainability kodu. Zawsze implementuj proper error handling i logging.
